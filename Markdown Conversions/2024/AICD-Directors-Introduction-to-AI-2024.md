# A Director’s Introduction to AI

## Table of Contents
- [How to use this guide](#how-to-use-this-guide)
- [Resource purpose, audience & structure](#resource-purpose-audience-structure)
- [Executive summary](#executive-summary)
- [Chapter 1: AI and the relevance for directors](#chapter-1-ai-and-the-relevance-for-directors)
    - [1.1 What is AI?](#11-what-is-ai)
        - [1.1.1 How is AI different from other technology?](#111-how-is-ai-different-from-other-technology)
        - [1.1.2 Different types of AI](#112-different-types-of-ai)
        - [1.1.3 How do I know when AI is being used in my organisation?](#113-how-do-i-know-when-ai-is-being-used-in-my-organisation)
    - [1.2 How and why is AI being used by organisations?](#12-how-and-why-is-ai-being-used-by-organisations)
- [Chapter 2: AI opportunities and risks](#chapter-2-ai-opportunities-and-risks)
    - [2.1 AI opportunities](#21-ai-opportunities)
    - [2.2 AI harms](#22-ai-harms)
        - [2.2.1 Harm to individuals and the importance of vulnerable communities](#221-harm-to-individuals-and-the-importance-of-vulnerable-communities)
        - [2.2.2 Harm to society](#222-harm-to-society)
        - [2.2.3 Harm to organisations](#223-harm-to-organisations)
    - [2.3 Key sources of AI risks and harms](#23-key-sources-of-ai-risks-and-harms)
    - [2.4 Perceptions of AI risk amongst corporate leaders](#24-perceptions-of-ai-risk-amongst-corporate-leaders)
- [Chapter 3: Current obligations and the evolving regulatory landscape](#chapter-3-current-obligations-and-the-evolving-regulatory-landscape)
    - [3.1 Current legal obligations in relation to the use of AI](#31-current-legal-obligations-in-relation-to-the-use-of-ai)
        - [3.1.1 Directors’ duties](#311-directors-duties)
        - [3.1.2 Existing legal obligations for organisations using AI](#312-existing-legal-obligations-for-organisations-using-ai)
    - [3.2 The evolving regulatory landscape](#32-the-evolving-regulatory-landscape)
        - [3.2.1 International trends](#321-international-trends)
        - [3.2.2 Australian regulatory and policy developments](#322-australian-regulatory-and-policy-developments)
    - [3.3 Safe and responsible AI principles](#33-safe-and-responsible-ai-principles)
- [Where to from here? Governance implications](#where-to-from-here-governance-implications)
- [Acknowledgements](#acknowledgements)

A Director’s 
Introduction 
to AI
Human
Technology
Institute
A JOINT PUBLICATION BY

# How to use this guide
Having considered all the boards on 
which you serve, select what applies to you:

What we 
suggest you read
- I know about ChatGPT, but I don’t know any other types of AI
- I am not clear how AI is different to other technologies
- I am unsure about the key legal obligations applying to AI use
- I am not clear about the key risks or opportunities arising from AI
- I do not know the underlying principles of safe and responsible AI
A Director's 
Introduction to AI
- I understand the difference between General AI and Narrow AI
- I understand how AI is different to other technologies, but am unclear how this 
impacts governance
- I am unsure about where AI is used within my organisation
- I am unsure about what questions to ask management about the governance 
and use of AI and how to assess the quality of management’s responses
A Director's Guide to 
AI Governance
- I am a director of a SME or NFP and do not know how to implement AI governance 
AI Governance 
Checklist for SME and 
NFP Directors
PAGE  3

# Resource purpose, audience & structure
This resource is intended to introduce directors to key AI concepts, and is structured into three chapters:
- Chapter 1 provides directors with an introduction to what AI is, how it is being used and its relevance for directors. 
- Chapter 2 outlines the key opportunities and risks of AI use.
- Chapter 3 examines current regulatory obligations related to AI systems and the shifting regulatory environment 
locally and internationally.

It is not intended to ‘cover the field’, but to develop a foundational understanding of AI governance. 
The resource lays the foundation for directors to apply AI governance principles. This is set out in Part 2 of this series, 
‘A Director’s Guide to AI Governance.’ 
PAGE  4

# Executive summary
There are two main types of AI systems: (1) 
General AI systems (which include Generative AI 
such as ChatGPT) and (2) Narrow AI systems.  
General AI and Narrow AI systems are subject to 
different risks and present different governance 
challenges. Both types of AI require additional 
consideration and oversight from management 
and directors
Managing AI systems can be particularly 
challenging because of their sophisticated pattern 
recognition capabilities, which operate at a large 
scale and pull from vast datasets to generate 
complex outputs. 
AI use within an organisation may not be obvious, 
which compounds the governance challenge. 
Increasingly, AI is being deployed in core 
organisational functions such as strategy, 
corporate finance and risk. This trend is likely to 
continue, increasing the need for boards to 
implement safe and responsible AI governance. 
Directors are ultimately responsible for the 
oversight of risk management throughout the 
organisation. This includes risk arising from AI.  
Existing legal obligations in the areas of privacy, 
consumer protection, intellectual property, cyber 
security, anti-discrimination, duty of care and 
work, health and safety continue to apply, and will 
be relevant to AI use. 
The regulatory landscape is evolving rapidly. While 
regulatory approaches differ, a common set of 
safe and responsible AI principles underpin 
reforms locally and internationally.
PAGE  5

# Chapter 1: AI and the relevance for directors
## 1.1  WHAT IS AI?
### 1.1.1  HOW IS AI DIFFERENT FROM OTHER TECHNOLOGY?
### 1.1.2  DIFFERENT TYPES OF AI
### 1.1.3  HOW DO I KNOW WHEN AI IS BEING USED IN MY ORGANISATION?
## 1.2  HOW AND WHY IS AI BEING USED BY ORGANISATIONS?
PAGE  6

KEY POINTS:
- There are two main types of AI systems: General 
AI (or General Purpose AI) and Narrow AI 
systems. They are subject to different risks and 
present different governance challenges.
- Managing AI systems can be particularly 
challenging because of their sophisticated 
pattern recognition capabilities, which operate 
at a large scale and pull from vast datasets to 
generate complex outputs. This can make their 
decisions difficult to explain.
- AI use within an organisation may 
not be obvious, which compounds the 
governance challenge.
- Increasingly, AI is being deployed in core 
organisational functions such as strategy, 
corporate finance and risk.  This trend is likely 
to continue, increasing the need for boards to 
implement safe and responsible AI governance.

## 1.1  WHAT IS AI?
The definition of AI adopted by the International 
Organisation for Standardization and the International 
Electrotechnical Commission ISO/IEC 22989 is: 
> An engineered system that generates outputs such as 
content, forecasts, recommendations or decisions for a 
given set of human-defined objectives. 

### 1.1.1  How is AI different from 
other technology?
AI is a special form of digital software that is particularly 
good at predicting outputs, optimising, classifying, 
inferring missing data, and generating new data. 
AI systems can often outperform traditional software, 
and as a result offer significant productivity, efficiency 
and customer experience benefits. 
AI is also more versatile and scalable than traditional 
software because it can be replicated and adapted to 
new contexts at a relatively low cost. As a result of these 
advantages, AI is increasingly being deployed across 
organisational teams and functions.
However, the differences between traditional 
software systems and AI systems also impacts 
governance approaches.
Traditional software systems are built from explicit 
rules coded by developers, such that their behaviour is 
inherently more predictable and understandable (even if 
the software itself is complex).
By contrast, AI systems are often created by defining an 
objective and using historical data to create an AI model 
that may rely on billions of inferred connections between 
data points to achieve its objective. This process means 
that it can be extremely challenging to replicate, 
explain or test an AI system’s output.

BOX 1: The role of data in AI systems
Data is the foundation of AI systems. Data, including 
personal information, is collected and used to train 
AI systems. It is both an input and an output of a 
deployed AI system. 
The selection of data, particularly its quality, 
quantity, and representativeness, will significantly 
affect the performance of AI systems. 
Through the ongoing collection of data and feedback 
loops, the accuracy and efficiency of AI systems  
should improve over time.  
PAGE  7

BOX 2: What kinds of systems are usefully defined as AI?
- Machine learning: a broad set of models that have been trained on pre-existing 
data to produce useful outputs on new data.
- Expert systems: systems that use a knowledge base, inference engine and logic to 
mimic how humans make decisions.
- Natural language systems: models that can understand and use natural 
language and speech for tasks such as summarisation, translation, or 
content moderation.
- Facial recognition technologies: systems that verify a person, identify someone, 
or analyse personal characteristics using facial data drawn from photos or video.
- Recommender systems: systems that suggest products, services or information 
to a user based on user preferences, characteristics, or behaviour.
- Automated decision-making systems: systems that use data to classify, analyse 
and make decisions that affect people with little or no human intervention.
- Robotic process automation: systems that imitate human actions to automate 
routine tasks through existing digital interfaces.
- Virtual agents and chatbots: digital systems that engage with customers or 
employees via text or speech.
- Generative AI: systems that produce code, text, music, or images based on text or 
other inputs.
- AI-powered robotics: physical systems that use computer vision and machine 
learning models to move and execute tasks in dynamic environments.

### 1.1.2  Different types of AI
Box 2 provides a non-exhaustive list of systems that meet the definition of AI above.
General AI (or General Purpose AI) and Narrow AI are two sub-categories of AI (see 
Table 1).

TABLE 1: Key differences between General AI and Narrow AI
| Type of AI system | Description[^1] | Examples |
|---|---|---|
| General AI (or General Purpose AI) | An AI system that can be used for a broad range of tasks, both intended and unintended by developers. This includes Generative AI. | Text generation (i.e. GPT-4, Gemini), image generation (i.e. DALL.E, Midjourney), programming code generation (i.e. Codex). |
| Narrow AI | An AI system trained to deliver outputs for specialised, constrained tasks and uses to address a specific problem. | Search engines (i.e. Google, Bing), facial recognition (i.e. Apple Face ID), recommender systems (i.e. Amazon, Spotify, Netflix). |

[^1]: ISO, 2022. ISO-IEC-22989 Artificial intelligence concepts and terminology. 

As discussed further in Chapter 2, General AI (including Generative AI) and Narrow AI 
present slightly different governance challenges (see Box 9).
PAGE  8

### 1.1.3  How do I know when AI is being used in my organisation?
AI use is not always obvious. This makes its use more difficult to govern. Box 3 sets out 
terms that may indicate that AI systems are in use within an organisation.
As AI advances rapidly, corporate leaders would be well-served to take a broad view of 
what constitutes an AI system within their organisation.

BOX 3: Key terms to listen out for to identify potential AI use within 
your organisation
The use of AI by organisations is not always clear to executives and directors. In 
addition to the kinds of systems listed in Box 2, common terms to listen for which 
may indicate the use of AI and warrant further investigation include:
- Model or algorithm (e.g. a specialised piece of software designed to provide a 
recommendation, optimise a system, or prioritise an action).
- Training data (e.g. data used to train or fine-tune an AI algorithm).
- Data analytics (e.g. a set of data transformations to classify consumer profiles).
- Predictive analytics (e.g. using data to predict future trends or events).
- Prescriptive analytics (e.g. analysing data to identify the optimal course 
of action).
- Process automation (e.g. the use of robotic process automation to perform 
repetitive tasks).
- Automated decision-making (e.g. the use of a set of rules or a self-learning 
algorithm to make a decision, such as providing a risk classification or approving 
a further action).

## 1.2  HOW AND WHY IS AI BEING USED BY ORGANISATIONS?
AI is rapidly becoming an essential part of how organisations operate. Research from 
Human Technology Institute (HTI) conducted with business leaders and directors in 
2023 found that almost two-thirds of Australian organisations are already using, or 
actively planning to use AI systems to support a wide variety of functions.
Organisations are introducing AI systems to secure a range of benefits, including:
- reducing costs;
- enhancing productivity;
- improving customer experience; and
- delivering new business growth.

HTI’s data indicates that non-executive directors tend to place more focus on the 
opportunity for AI systems to serve customers better, while managers tend to see 
greater value in deploying AI systems to achieve process efficiencies (Figure 1).

![Figure 1: Top expected benefits of AI use by Business Leaders (BL) and Non-executive Directors (NED) surveyed by HTI in 2023]

While Narrow AI systems have traditionally been the domain of data analytics teams, cyber security systems or other 
back-office functions, the rising capabilities and flexibility of AI systems mean they are increasingly being used in 
ways that touch stakeholders directly.
AI systems are undergoing significant changes in their application. Three of the top five priority areas for AI system 
use directly impact consumers or employees, including customer service, marketing and sales, and human resources.[^2]
[^2]: Lauren Solomon and Nicholas Davis, The State of AI Governance in Australia (HTI Report, 2023). 

AI is becoming increasingly used in three key areas:
1. IMPROVING THE CUSTOMER AND 
EMPLOYEE EXPERIENCE
AI is being used to extend and augment the reach 
and output of employees in a way that can improve 
the customer experience and reduce the drudgery 
of mundane tasks (thereby freeing up employees for 
higher value-add work). We detail some of the benefits 
of AI use in Chapter 2 (section 2.1). For example, 
Telstra is using Generative AI systems to support 
frontline teams and answer complex customer queries, 
while an AI-driven dashboard in NSW hospital 
emergency rooms helps doctors identify patients at a 
high risk of sepsis.
2. INCORPORATION INTO NEW PRODUCTS AND 
SERVICES THROUGHOUT THE VALUE CHAIN
AI is being bundled into products and services 
that organisations procure through technology 
partners. This means it is often used by employees 
and across supply chains in ways that are often not 
fully visible. For example, a February 2024 survey of 
1,000 office workers commissioned by Salesforce 
found that 53 per cent of Australian professionals are 
actively using or experimenting with Generative AI at 
work. Not all of this employee use of AI is disclosed 
(known as ‘Shadow IT’ or ‘shadow AI use’(see Box 3 in 
Section 1.3 of A Director's Guide to AI Governance)), 
which creates risks and governance challenges.
3. INCORPORATION INTO CORE 
BUSINESS FUNCTIONS
AI systems are being applied closer to the ‘core’ of 
organisations, with some of the most rapid growth 
in strategy, corporate finance, and risk functions. 
For example, a 2024 NVIDIA survey found that risk 
management was the second-highest current use and 
top investment domain for AI systems in the financial 
services sector.
PAGE  10

# Chapter 2: AI opportunities and risks
## 2.1  AI OPPORTUNITIES
## 2.2  AI HARMS
### 2.2.1  HARM TO INDIVIDUALS AND THE IMPORTANCE OF VULNERABLE COMMUNITIES
### 2.2.2  HARM TO SOCIETY
### 2.2.3  HARM TO ORGANISATIONS
## 2.3  KEY SOURCES OF AI RISKS AND HARMS
## 2.4  PERCEPTIONS OF AI RISK AMONGST CORPORATE LEADERS
PAGE  11

## 2.1  AI OPPORTUNITIES
AI systems promise a range of significant benefits for organisations. These include:
- INCREASED EFFICIENCY AND PRODUCTIVITY
Some AI systems can reduce the time burden 
of administrative tasks through new forms of 
automation. Others allow employees to expand 
their output and add additional value, helping 
teams to analyse trends, summarise existing 
content, and generate new content. A 2023 
experiment designed by MIT Sloan found that 
when Generative AI was particularly suited to 
a task, it could enhance worker productivity by 
approximately 40 per cent.
- REDUCTION IN ERROR AND QUALITY 
IMPROVEMENTS
While AI systems are extremely prone to errors 
when input data or queries fall outside their 
core competency, for well-known mechanical 
or repetitive tasks, particularly those involving 
pattern recognition, they can perform 
significantly better than other approaches, 
including human experts.
- NEW PRODUCTS AND SERVICES
Both Narrow and General AI systems can support 
organisations with a range of innovation-related 
tasks, including helping organisations identify, 
predict demand, design, prototype, and test new 
products and services.
- IMPROVED CUSTOMER EXPERIENCE
Thanks to their ability to engage in natural 
language and scale digitally, General AI systems 
are helping to reduce customer wait times, 
improve accessibility of existing information, and 
personalise the customer experience.
- IMPROVED EMPLOYEE EXPERIENCE
AI can reduce the time and cost spent by 
employees on administrative tasks to allow focus 
on value-add work and innovation. Some AI 
(such as Generative AI) can also guide workers 
through more complex tasks and can assist 
in problem-solving.

KEY POINTS:
- AI use can produce benefits and opportunities 
as well as risks and harms.
- Key benefits of using AI systems include 
increased productivity, quality improvement, 
new products and services, and an improved 
customer and employee experience.
- Many of the potential harms to consumers or 
employees from AI system misuse or failure are 
foreseeable and capable of mitigation.
- Without appropriate controls, AI systems tend 
to negatively and disproportionately affect 
vulnerable and marginalised populations. 
Organisations need to ensure that processes 
are in place to identify and prevent 
these harms.
PAGE  12

Capitalising on the opportunities above requires an investment of time and resources. 
Successful implementation of AI also relies on high levels of trust and engagement 
from customers and employees, supporting infrastructure (including effective data 
governance), and users with the necessary skills and training.
Conversely, inaction or failing to seize the opportunities offered by AI can present 
significant risks for organisations (see section 2.2.3). Early adopters of AI systems 
have gained, and are continuing to gain, competitive advantages.[^3] For example, AI-
driven search, pioneered by Google, signficantly impacted advertising strategies, and 
AI systems were central to the disruption of the taxi industry by rideshare companies.
Sectoral differences are also emerging in AI adoption and use. In Australia, research 
indicates that aerospace, defence and security, mining, energy and resources, 
agriculture, health, and transport were the top industries serviced by AI firms in 2021.[^4] 
Globally, the industries leading the adoption of AI technology in 2023 were technology, 
financial services, health, transport and education.[^5]
[^3]: McKinsey, The state of AI in 2022 — and a half decade in review (Report, December 2022).
[^4]: Austrade, The 2021 Australian Artificial Intelligence Export Survey (Report, 2021). 
[^5]: John Mangan, Australia’s AI Imperative: The economic impact of artificial intelligence and what’s needed to further its growth (Kingston AI Group Report, 2024). 

BOX 4: Potential economic impact of Generative AI on the 
Australian economy
In July 2023, Microsoft and the Tech Council of Australia issued a report on the 
economic impact of Generative AI on Australia.
The report found that Generative AI could add between $45 to $115 billion in GDP 
to the Australian economy annually.
The majority (70 per cent) of these gains are estimated to come from productivity 
improvements - it is estimated that Generative AI has the potential to automate or 
augment 44 per cent of an average worker's tasks. The remainder comes from new 
jobs and new products, services and businesses, with the biggest opportunities in 
healthcare, manufacturing, retail and professional and financial services.
PAGE  13

## 2.2  AI HARMS
The specific characteristics of AI systems that set them apart from traditional software also mean that they 
can amplify existing harms while creating new ones that may affect individuals, organisations and/or society. 
Table 2 summarises these potential harms.

TABLE 2: Potential harms to individuals, organisations and society from AI systems
| Harm to individuals (consumers, employees, members of the public) | Harm to organisations | Harm to society |
|---|---|---|
| - Physical, psychological, economic, or reputational harm  - Misleading advice or information  - Violation of civil liberties  - Breach of privacy  - Unlawful discrimination and exclusion  - Unfair treatment | - Commercial losses  - Reputational damage  - Regulatory sanctions | - Job displacement  - Economic inequality  - Large-scale damage to public health, infrastructure or essential services  - Environmental damage  - Social and political manipulation  - Discrimination and oppression of minority groups |
PAGE  14

### 2.2.1  Harm to individuals and the importance of 
vulnerable communities
As Chapter 3 details, there are a wealth of existing laws relevant to an organisation’s 
use of AI. Some of these may result in liability for organisations – and directors 
personally – if individuals are harmed as a result of AI systems.
Research indicates that AI harms tend to disproportionately affect vulnerable and 
marginalised communities. The reasons for this are complex, but are often driven by 
systemic biases that exist in the data used to train AI models, the design, engineering 
and modelling processes, and the contexts in which AI models are ultimately deployed 
by decision makers.[^6] 
The under representation of women or people of colour in data can lead to decreased 
accuracy of AI systems, such as facial recognition technologies or computer-aided 
diagnosis systems, including medical image interpretation. 
Directors should be mindful of the impact of AI use on vulnerable and marginalised 
individuals, and consider ways to mitigate this (see practical steps in A Director’s 
Guide to AI Governance).
[^6]: Reva Schwartz et al, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence (NIST Special Publication 1270, 2022). 

BOX 5: The Risk of Bias
Bias is one of the most well-documented concerns related to AI systems. This issue 
arises because of the potential for AI systems to inherit and amplify biases present 
in real-world data.
AI systems are also prone to bias due to their reliance on historical data for 
training. Bias may emerge from: 
- pre-existing biases present in the real world; 
- the use of non-representative data sets; and 
- the selection of algorithm approaches or objective functions that intrinsically 
embed the bias of the development team.
Directors should be vigilant that the use of an AI system may create outcomes that 
are unlawful and discriminatory and disadvantage individuals or groups based on 
protected characteristics such as their age, race, sex, or disability.
Addressing bias in AI systems is not straightforward. Technical solutions alone are 
often insufficient to fully rectify the biases embedded within real-world data. The 
Australian Human Rights Commission’s technical paper on addressing algorithmic 
bias provides a comprehensive examination of this issue, highlighting the 
limitations of relying solely on technological fixes to address biases in AI systems.
In light of this, directors should be aware of, and champion approaches that 
combine technical, operational, and governance practices to help mitigate 
the risk of bias and ensure AI systems are developed and deployed in a fair and 
human-centred manner.
PAGE  15

### 2.2.2  Harm to society
Collective harms that can arise from AI system misuse 
or failure include social and political manipulation, new 
forms of technological unemployment, and the systemic 
oppression or exclusion of minority groups. When used at 
scale by those with broad reach (such as governments 
and essential service providers) even relatively small 
errors or biases in a system can cause large harms when 
scaled across groups.
While such society-wide effects are the purview of 
government policy, directors should be aware of these 
macro-level harms and how issues such as AI system job 
displacement may be viewed by stakeholders.
[^7]: Jan Hatzius et al, The Potentially Large Effects of Artificial Intelligence on Economic Growth (Briggs/Kodnani), Goldman Sachs (online, 26 March 2023).

BOX 6: Workforce impact and job displacement
Research estimates that 300 million full-time equivalent workers are susceptible to automation as a result of AI 
systems.[^7] However, the research finds AI systems are more likely to augment workers by automating some tasks, 
but not replace them outright. Further, AI is creating new jobs, bringing opportunities for the retraining and 
redeployment of workers.
A World Economic Forum report states that AI is expected to be adopted in 2023-2027 by 75 per cent of 
companies surveyed. Despite the workforce transformations that many anticipate being driven by AI, only 25 per 
cent of these organisations expect it to create net job losses. More than 50 per cent of organisations expect it to 
create net job growth.
As AI transforms the way people work, it also offers the possibility of improving worker satisfaction. A survey 
by Microsoft indicates that whilst 49 per cent of people are worried AI will replace their jobs, 70 per cent would 
happily delegate work to AI to ease their workloads.
Organisations should engage with employees about the impact of AI on their roles and the potential for AI to 
assist worker productivity and efficiency and to provide skills to allow them to gain opportunities for retraining.
PAGE  16

### 2.2.3  Harm to organisations
It is important to recognise that there are risks for organisations at two levels: 
- risks arising from AI system investment and use; and
- risks arising from underinvestment and a lack of adoption.

AI misuse or system failures can create and amplify a range of commercial, 
reputational and regulatory risks to organisations.

![Figure 2: Risks to organisations from AI use]

On the other hand, a lack of investment in AI capabilities also leaves organisations 
vulnerable to a range of other risks, such as a lack of competitiveness, higher costs, 
lack of new product and service delivery, poorer consumer service, as well as talent 
acquisition and retention challenges.
The risks of action and inaction must be carefully weighed by directors alongside the 
organisational strategy and the risk appetite of the organisation (discussed further in 
A Director’s Guide to AI Governance).

BOX 7: Sustainability, ESG and AI
AI systems raise risks and harms that may be captured under environmental, social 
and governance (ESG) or sustainability frameworks. For example, the training of 
Generative AI models can have an environmental impact given the significant 
energy and water it requires. The potential of AI to disproportionately impact 
vulnerable persons is also a ‘social’ risk (the 'S' within ESG).
Addressing ESG matters will necessarily address some of the risks and harms of 
AI systems.
In November 2022, AICD partnered with Herbert Smith 
Freehills to publish, under the Climate Governance 
Initiative (CGI) Australia banner, a ‘Bringing together 
ESG’ resource to help boards develop appropriate 
governance structures to effectively oversee 
sustainability issues.
However, given the specific challenges, harms and opportunities of AI systems, 
ESG frameworks alone are not sufficient. There must be broader consideration of AI 
systems for their effective governance (see A Director’s Guide to AI Governance).

BOX 8: Generative AI and cyber security risks
Generative AI systems are likely to give rise to specific cyber security risks and 
undermine existing controls. For example, deepfake AI tools could be used to 
generate realistic synthetic media that impersonates individuals for the purposes of 
identity theft, fraud or spreading misinformation about a particular organisation. 
To mitigate these risks, boards should oversee the strengthening of cyber security 
controls, such as data encryption, access controls, employee training and regular 
external cyber security audits.
PAGE  17

## 2.3  KEY SOURCES OF AI RISKS AND HARMS
AI risks and harms are created because of the way AI systems perform and behave, 
as well as how they might be used. Table 3 outlines some examples of how such 
risks arise.

TABLE 3: Key sources of AI risk for organisations
| Key sources of AI risk | Examples |
|---|---|
| AI system failures – where systems create harm because they fail to perform as intended | - Poor system performance  - Biased system performance  - System fragility or unreliability  - Security failures or vulnerabilities |
| Malicious, misleading, reckless, or inappropriate use – where systems are deliberately used (whether by the organisation or external parties) in a way which creates or amplifies risk of harm | - Misleading advice  - Misinformation at scale  - Unfair or extractive use  - Opacity and lack of interpretability  - Weaponisation  - AI-powered cyber attacks  - Fraudulent and unlawful use e.g. scams  - Financial market manipulation  - Excessive deployment  - Deployment on vulnerable individuals |

Risk management frameworks must identify and mitigate risks of system failures and 
misuse – this is discussed further in A Director’s Guide to AI Governance.
Table 4 sets out five characteristics that impact the potential for an AI system to cause 
harm. Box 9 shows how these five characteristics create different levels of challenge 
across Generative AI (a subset of General AI) and Narrow AI systems.

TABLE 4: Factors that drive harms from system failure, misuse, or 
inappropriate use
| Factor | Relevance to harms |
|---|---|
| Purpose | An AI system’s potential for harm is sensitive to its intended and actual purpose. If a system is used for a highly consequential purpose – for example, an output that has a legal or similarly significant effect, or a system that controls critical infrastructure – its potential to cause harm will be elevated. |
| Context | The potential for harm is also a function of the individuals or groups with whom the system interacts. The risk of harm is elevated if vulnerable individuals, such as children or minorities who are subject to, or require, additional forms of legal protection, are exposed to a system or are the subjects of its outputs. |
| Data | The risk of harm will rise when confidential, personal or sensitive information is used in an AI system’s training or operation, data quality or provenance is unclear or unverifiable, if live data is ingested, and if large amounts of data are used. |
| Technical architecture | The choice of AI model or software, the surrounding system elements, and the quality of supporting infrastructure will also influence the risk of harm from an AI system. For example, the fact that Generative AI systems can be induced to produce content that is inappropriate in a work environment makes them inherently more risky than alternatives. |
| Level of automation | The potential for harm rises when an AI system is triggered automatically or produces outputs that are fed into other systems with little or no human verification, particularly when tied to a consequential purpose. |
PAGE  18

BOX 9: Emergent challenges of Generative AI systems (HTI, 2024)
|  | Generative AI | Narrow or traditional AI |
|---|---|---|
| Purpose: Range of use | Generative AI systems are being deployed for a wide array of different tasks, but will not give accurate answers to queries outside their core training knowledge. | Narrow AI systems tend to be specifically trained and deployed within tight boundaries and controlled conditions. |
| Context: User familiarity and training | Generative AI systems are increasingly used by employees outside of technical teams, and require special training to be used appropriately. Employees may also be using these systems in ways unintended or permitted by their organisation (known as ‘shadow’ AI use). | Narrow AI systems tend to be designed and used by a small group of technical experts. |
| Data: Intellectual property | Systems often contain and may reproduce embedded, copyright-protected training data. | IP risks apply to training data, but input data to Narrow AI systems is often more knowable and manageable. |
| Data: Confidentiality and personally identifiable information | Systems may contain and reproduce confidential information or personally identifiable information including data inputted by users. | Data protection and confidentiality risks relate primarily to how data flows through the system. |
| Data: Quality and provenance | The scale of training data means quality can be dubious. Data quality can compromise fine-tuning. Low-quality prompts can compromise outputs. | Data provenance, quality and management are critical, but more knowable. |
| Data: Bias and fairness | Complex, real-world bias is often deeply embedded in models. | Bias is heavily reliant on data representativeness and algorithm choice. |
| Technical architecture: Potential for misuse | Deliberate, spontaneous, or user-induced production of harmful, misleading or manipulative content. | Misuse or misleading use of Narrow AI systems is possible, but easier to prevent. |
| Technical architecture: Accuracy | ‘Hallucinations’ (namely, coherent responses that are false or incorrect) are common in Generative AI systems, particularly in response to prompts asking for information outside core training data. | The accuracy of Narrow AI systems can vary widely, but is generally more consistent and known. |
| Technical architecture: Reproducibility | Generative AI systems tend to produce very different results from similar input. Randomness is often deliberately used to improve output quality. | Narrow AI systems tend to be more predictable and stable, allowing for more systematic testing. |
